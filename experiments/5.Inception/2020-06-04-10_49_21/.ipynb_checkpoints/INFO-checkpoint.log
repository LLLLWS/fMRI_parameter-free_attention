2020-06-04 10:49:23,582[line:2] - INFO: --------------------------------------------------------------
2020-06-04 10:49:23,584[line:4] - INFO: Network loading, learningRate: 0.0001, BatchSize: 16, experimentTimes: 0
2020-06-04 10:49:23,597[line:15] - INFO: DataLoader have been created!
2020-06-04 10:49:27,998[line:18] - INFO: Network have been established.
2020-06-04 10:49:28,002[line:24] - INFO: Train begining.
2020-06-04 10:50:43,907[line:3] - INFO: 5: (train) acc:18.75%, loss:2.0123674869537354
2020-06-04 10:50:51,744[line:3] - INFO: 10: (train) acc:6.25%, loss:2.0377607345581055
2020-06-04 10:50:59,517[line:3] - INFO: 15: (train) acc:25.0%, loss:1.907740592956543
2020-06-04 10:51:07,389[line:3] - INFO: 20: (train) acc:31.25%, loss:1.796338438987732
2020-06-04 10:51:15,230[line:3] - INFO: 25: (train) acc:50.0%, loss:1.7735304832458496
2020-06-04 10:51:23,361[line:3] - INFO: 30: (train) acc:25.0%, loss:1.7473750114440918
2020-06-04 10:51:23,369[line:45] - INFO: Evaluating!
2020-06-04 10:53:11,026[line:16] - INFO: Evaluationg, 16
2020-06-04 10:53:49,824[line:54] - ERROR: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.91 GiB total capacity; 11.09 GiB already allocated; 149.06 MiB free; 11.20 GiB reserved in total by PyTorch)
2020-06-04 10:55:39,427[line:3] - INFO: 35: (train) acc:12.5%, loss:1.7998162508010864
2020-06-04 10:55:47,086[line:3] - INFO: 40: (train) acc:31.25%, loss:1.7196698188781738
2020-06-04 10:55:54,745[line:3] - INFO: 45: (train) acc:56.25%, loss:1.6954458951950073
2020-06-04 10:56:02,515[line:3] - INFO: 50: (train) acc:37.5%, loss:1.8283442258834839
2020-06-04 10:56:10,399[line:3] - INFO: 55: (train) acc:43.75%, loss:1.6347391605377197
2020-06-04 10:56:18,268[line:3] - INFO: 60: (train) acc:37.5%, loss:1.676297664642334
2020-06-04 10:56:18,286[line:45] - INFO: Evaluating!
2020-06-04 10:58:10,007[line:16] - INFO: Evaluationg, 16
2020-06-04 10:58:43,129[line:54] - ERROR: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.91 GiB total capacity; 10.75 GiB already allocated; 149.06 MiB free; 11.20 GiB reserved in total by PyTorch)
2020-06-04 11:00:05,949[line:3] - INFO: 65: (train) acc:56.25%, loss:1.4273364543914795
2020-06-04 11:00:13,884[line:3] - INFO: 70: (train) acc:43.75%, loss:1.4409806728363037
2020-06-04 11:00:21,734[line:3] - INFO: 75: (train) acc:12.5%, loss:1.8545902967453003
2020-06-04 11:00:29,696[line:3] - INFO: 80: (train) acc:18.75%, loss:1.6440857648849487
2020-06-04 11:00:37,631[line:3] - INFO: 85: (train) acc:31.25%, loss:1.4980897903442383
2020-06-04 11:00:45,642[line:3] - INFO: 90: (train) acc:37.5%, loss:1.5209437608718872
2020-06-04 11:00:45,661[line:45] - INFO: Evaluating!
2020-06-04 11:02:39,368[line:16] - INFO: Evaluationg, 16
2020-06-04 11:03:26,556[line:54] - ERROR: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.91 GiB total capacity; 10.75 GiB already allocated; 149.06 MiB free; 11.20 GiB reserved in total by PyTorch)
2020-06-04 11:05:03,200[line:3] - INFO: 95: (train) acc:18.75%, loss:1.6596174240112305
2020-06-04 11:05:11,306[line:3] - INFO: 100: (train) acc:56.25%, loss:1.3938947916030884
2020-06-04 11:05:19,341[line:3] - INFO: 105: (train) acc:43.75%, loss:1.4854027032852173
2020-06-04 11:05:27,255[line:3] - INFO: 110: (train) acc:56.25%, loss:1.5205225944519043
2020-06-04 11:05:35,155[line:3] - INFO: 115: (train) acc:31.25%, loss:1.6802772283554077
2020-06-04 11:05:43,162[line:3] - INFO: 120: (train) acc:25.0%, loss:1.4515190124511719
2020-06-04 11:05:43,172[line:45] - INFO: Evaluating!
2020-06-04 11:07:26,845[line:16] - INFO: Evaluationg, 16
2020-06-04 11:08:24,603[line:54] - ERROR: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.91 GiB total capacity; 10.75 GiB already allocated; 149.06 MiB free; 11.20 GiB reserved in total by PyTorch)
2020-06-04 11:09:56,694[line:3] - INFO: 125: (train) acc:37.5%, loss:1.333404779434204
2020-06-04 11:10:04,614[line:3] - INFO: 130: (train) acc:50.0%, loss:1.635964274406433
2020-06-04 11:10:12,590[line:3] - INFO: 135: (train) acc:43.75%, loss:1.360840916633606
2020-06-04 11:10:20,702[line:3] - INFO: 140: (train) acc:43.75%, loss:1.513972282409668
2020-06-04 11:10:28,657[line:3] - INFO: 145: (train) acc:37.5%, loss:1.3394768238067627
2020-06-04 11:10:36,714[line:3] - INFO: 150: (train) acc:43.75%, loss:1.5920860767364502
2020-06-04 11:10:36,719[line:45] - INFO: Evaluating!
2020-06-04 11:12:13,132[line:16] - INFO: Evaluationg, 16
2020-06-04 11:13:13,062[line:54] - ERROR: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.91 GiB total capacity; 10.75 GiB already allocated; 149.06 MiB free; 11.20 GiB reserved in total by PyTorch)
2020-06-04 11:14:39,050[line:3] - INFO: 155: (train) acc:37.5%, loss:1.731994390487671
2020-06-04 11:14:47,034[line:3] - INFO: 160: (train) acc:50.0%, loss:1.236282229423523
2020-06-04 11:14:55,156[line:3] - INFO: 165: (train) acc:31.25%, loss:1.4318933486938477
2020-06-04 11:15:03,179[line:3] - INFO: 170: (train) acc:37.5%, loss:1.3192490339279175
2020-06-04 11:15:11,115[line:3] - INFO: 175: (train) acc:31.25%, loss:1.704668402671814
2020-06-04 11:15:19,166[line:3] - INFO: 180: (train) acc:62.5%, loss:1.2096902132034302
2020-06-04 11:15:19,173[line:45] - INFO: Evaluating!
2020-06-04 11:17:05,038[line:16] - INFO: Evaluationg, 16
2020-06-04 11:17:53,220[line:54] - ERROR: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.91 GiB total capacity; 10.75 GiB already allocated; 149.06 MiB free; 11.20 GiB reserved in total by PyTorch)
2020-06-04 11:19:34,565[line:3] - INFO: 185: (train) acc:50.0%, loss:1.2280192375183105
2020-06-04 11:19:42,467[line:3] - INFO: 190: (train) acc:50.0%, loss:1.3660989999771118
2020-06-04 11:19:50,108[line:3] - INFO: 195: (train) acc:50.0%, loss:1.1895402669906616
2020-06-04 11:19:57,849[line:3] - INFO: 200: (train) acc:37.5%, loss:1.5202555656433105
2020-06-04 11:20:05,972[line:3] - INFO: 205: (train) acc:43.75%, loss:1.1607856750488281
2020-06-04 11:20:13,967[line:3] - INFO: 210: (train) acc:62.5%, loss:1.0737676620483398
2020-06-04 11:20:13,971[line:45] - INFO: Evaluating!
2020-06-04 11:22:03,830[line:16] - INFO: Evaluationg, 16
2020-06-04 11:22:42,424[line:54] - ERROR: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.91 GiB total capacity; 10.75 GiB already allocated; 149.06 MiB free; 11.20 GiB reserved in total by PyTorch)
2020-06-04 11:24:29,604[line:3] - INFO: 215: (train) acc:68.75%, loss:1.0112240314483643
2020-06-04 11:24:37,235[line:3] - INFO: 220: (train) acc:43.75%, loss:1.328309416770935
2020-06-04 11:24:44,987[line:3] - INFO: 225: (train) acc:50.0%, loss:1.0299321413040161
2020-06-04 11:24:52,638[line:3] - INFO: 230: (train) acc:56.25%, loss:1.466318130493164
2020-06-04 11:25:00,350[line:3] - INFO: 235: (train) acc:68.75%, loss:0.8820095062255859
2020-06-04 11:25:08,332[line:3] - INFO: 240: (train) acc:81.25%, loss:0.9597087502479553
2020-06-04 11:25:08,356[line:45] - INFO: Evaluating!
2020-06-04 11:27:02,460[line:16] - INFO: Evaluationg, 16
2020-06-04 11:27:34,467[line:54] - ERROR: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.91 GiB total capacity; 10.75 GiB already allocated; 149.06 MiB free; 11.20 GiB reserved in total by PyTorch)
2020-06-04 11:29:00,347[line:3] - INFO: 245: (train) acc:50.0%, loss:1.3010541200637817
2020-06-04 11:29:07,961[line:3] - INFO: 250: (train) acc:87.5%, loss:0.9604935646057129
2020-06-04 11:29:15,758[line:3] - INFO: 255: (train) acc:68.75%, loss:1.2035071849822998
2020-06-04 11:29:23,630[line:3] - INFO: 260: (train) acc:68.75%, loss:0.8950999975204468
2020-06-04 11:29:31,612[line:3] - INFO: 265: (train) acc:87.5%, loss:0.6889842748641968
2020-06-04 11:29:39,537[line:3] - INFO: 270: (train) acc:68.75%, loss:1.0244498252868652
2020-06-04 11:29:39,556[line:45] - INFO: Evaluating!
